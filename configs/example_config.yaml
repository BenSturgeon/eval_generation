general_params:
  use_cache: True
  refresh_cache: False
  problem_types: 
    - goal_conflict
  
generation_params:
  model: gpt-4.1
  temperature: 1.5
  max_tokens: 1500
  top_p: 0.95
  n_prompts: 200
  n_prompts_created_per_generation: 1


QA_params:
  model: o3-2025-04-16
  n_relevant_prompts: 140

diversity_params:
  model: text-embedding-3-small
  n_diverse_prompts: 100

evaluation_params:
  refresh_cache: gpt-4.1-mini
  evaluator_model: o3-2025-04-16
  subject_models: 
    - gpt-4.1-mini
    # - gpt-4o
    # - gpt-4.1
    # - o3-2025-04-16
    # - o3-mini-2025-01-31
    # - o4-mini-2025-04-16
    # - claude-3-7-sonnet-20250219
    # - claude-3-5-sonnet-20240620
    # - claude-3-opus-20240229
    # - claude-3-haiku-20240307
    # - claude-3-5-haiku-20241022
    # - claude-3-5-sonnet-20241022
    # - models/gemini-2.0-flash
    # - models/gemini-1.5-flash
    # - models/gemini-2.5-pro-preview-03-25
    # - models/gemini-2.5-flash-preview-04-17
    # - grok-3
    # - meta/meta-llama-3-70b-instruct
    # - meta/llama-4-scout-instruct
    # - meta/llama-4-maverick-instruct



# precise model dates used:
    # - gpt-4.1-mini-2025-04-14
    # - gpt-4o-2024-08-06
    # - gpt-4.1-2025-04-14  
    # - o3-mini-2025-01-31
    # - o4-mini-2025-04-16

  subject_model_temperature: 0
  subject_model_top_p: 0.95
  subject_max_tokens: 1000
  gemini_max_tokens: 8192 # Special token limit for specific Gemini models
  evaluator_max_tokens: 5000

